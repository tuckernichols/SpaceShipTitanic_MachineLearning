{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c85487",
   "metadata": {},
   "source": [
    "# SPACE SHIP TITANIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae9bcd",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a81bcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb04932",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Separation\n",
    "\n",
    "load the training and testing data, set 'PassengerId' as the index, and split the training data into data (X) and target (y). original 'PassengerId'  is saved for the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c2043056",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_full = pd.read_csv(\"train.csv\", index_col = \"PassengerId\")\n",
    "test_data = pd.read_csv(\"test.csv\",index_col= \"PassengerId\")\n",
    "\n",
    "ID = test_data.index # for sumbission at end\n",
    "\n",
    "y = Data_full.Transported\n",
    "X = Data_full.drop(\"Transported\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc47a9",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering: Cabin\n",
    "\n",
    "extract features from the 'Cabin' column. splits 'Cabin' into 'Deck', 'CabinNumber' and 'Side', maps the 'Deck' letters to numerical values, 'CabinNumber' is scaled in transformer, and \"side\" is maped to boolean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5c343f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column creation and manipulation\n",
    "\n",
    "DeckMap = {'G':7, 'F':6, 'E':5, 'D':4, 'C':3, 'B':2, 'A':1, 'T':0}\n",
    "\n",
    "for data_set in [X, test_data]:\n",
    "    data_set[['Deck', 'CabinNumber', 'PortSide']] = data_set['Cabin'].str.split('/', expand=True)\n",
    "\n",
    "    data_set[\"PortSide\"]= data_set[\"PortSide\"].map({\"p\":True, \"S\":False, })\n",
    "    data_set[\"Deck\"] = data_set[\"Deck\"].map(DeckMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749ef79",
   "metadata": {},
   "source": [
    "## Data Selection and Split\n",
    "\n",
    "features are selected from all X data. data is split into train / test (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "057c520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "\n",
    "features = [\"Age\", \"CryoSleep\", \"VIP\", 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', \"HomePlanet\",\"Destination\", 'Deck', 'CabinNumber', 'PortSide']\n",
    "X = X[features]\n",
    "X_train, X_valid, y_train, y_valid = sklearn.model_selection.train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e19b1",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipelines\n",
    "\n",
    "This section sets up preprocessing for different types of features in the dataset using scikit-learn Pipelines and a ColumnTransformer. Each type of column has a tailored transformation:\n",
    "\n",
    "- **Spending columns** (`RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`):  \n",
    "  - Missing values are filled with 0\n",
    "  - Values are scaled with StandardScaler\n",
    "\n",
    "- **Numerical columns** (`Age`, `CabinNumber`):  \n",
    "  - Missing values are filled with the **mean**.  \n",
    "  - scaled using `StandardScaler`.\n",
    "\n",
    "- **Boolean columns** (`CryoSleep`, `VIP`, `PortSide`):  \n",
    "  - Missing values filled with the **most frequent value** (mode).  \n",
    "  - not one-hot encoded.\n",
    "\n",
    "- **Categorical columns** (`Destination`, `HomePlanet`):  \n",
    "  - Missing values filled with the **most frequent** value.  \n",
    "  - one-hot encoded.\n",
    "\n",
    "- **Deck column** (`Deck`):  \n",
    "  - Missing values filled with the median deck.  \n",
    "  - scaled using `StandardScaler`.\n",
    "\n",
    "Finally, all these transformations are combined in a **`ColumnTransformer`** (`preprocessor`) so that each type of feature is automatically processed when passed through the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1ee22269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "spending_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "numericall_cols = [\"Age\", \"CabinNumber\"]\n",
    "bool_cols = [\"CryoSleep\", \"VIP\", \"PortSide\"]           # True/False mode imputation only\n",
    "catgorial_cols = [\"Destination\", \"HomePlanet\"]\n",
    "Median_cols = [\"Deck\"]\n",
    "\n",
    "spending_transformer = Pipeline([   # [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "numericall_transformer = Pipeline([        # [\"Age\", \"CabinNumber\"]\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "boolean_transformer = Pipeline([    #[\"CryoSleep\", \"VIP\", \"PortSide\"]     \n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))    # no one_hot_encode (try one hot encode later)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([    # [\"Destination\", \"HomePlanet\"]\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "median_transformer = Pipeline([    # [\"Deck\"]\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('spending', spending_transformer, spending_cols),\n",
    "    ('age', numericall_transformer, numericall_cols),\n",
    "    ('bool', boolean_transformer, bool_cols),\n",
    "    ('cat', categorical_transformer, catgorial_cols),\n",
    "    ('median', median_transformer, Median_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c0eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:\n",
      "19.871205151793927\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "Model = Pipeline([\n",
    "    ('scaler', sklearn.preprocessing.StandardScaler()),\n",
    "    ('model', xgb.XGBClassifier(\n",
    "        n_estimators= 300,\n",
    "        learning_rate=0.15,          # 300 0.15, 2   \n",
    "        max_depth=2,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', Model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_preds = pipeline.predict(X_valid)\n",
    "print(\"MAE:\")\n",
    "print(sklearn.metrics.mean_absolute_error(y_valid, y_preds) * 100)\n",
    "\n",
    "# runs \n",
    "# 1 : 31.4543\n",
    "# 2: 28.408\n",
    "# 3: 27.7449\n",
    "# 4: 27.9839\n",
    "# 5: 21.3891\n",
    "# 6: 20.5151\n",
    "# 7: 19.50321\n",
    "# 8: 17.7552    SUMBIT: 0.7977\n",
    "# 9: 19.8712    SUMBIT: 0.7928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "01c1b883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbest params: {'model__model__learning_rate': 0.09, 'model__model__max_depth': 2, 'model__model__n_estimators': 200}\n",
      "best accuracy: 0.7933989922703885\n"
     ]
    }
   ],
   "source": [
    "# optimize model params\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'model__model__n_estimators': [50, 100, 150, 200, 250, 350],\n",
    "    'model__model__learning_rate': [0.09, 0.11, 0.13, 0.15, 0.17, 0.2],\n",
    "    'model__model__max_depth': [2, 3, 4],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy')  # use accuracy for classification\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"bbest params:\", grid_search.best_params_)\n",
    "print(\"best accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3db0b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train Model on all data\n",
    "pipeline.fit(X, y)\n",
    "test_preds = pipeline.predict(test_data)\n",
    "\n",
    "# generate sumbision\n",
    "test_data[\"PassengerId\"] = id\n",
    "test_preds = test_preds.astype(bool)\n",
    "\n",
    "output = pd.DataFrame({\"PassengerId\" : ID, \"Transported\": test_preds})\n",
    "sumbission_file_name = f\"submission_{pd.Timestamp.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "output.to_csv(sumbission_file_name, index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
